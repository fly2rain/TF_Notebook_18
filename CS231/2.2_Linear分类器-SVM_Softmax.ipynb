{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Linear-Classification\" data-toc-modified-id=\"Linear-Classification-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Linear Classification</a></span></li><li><span><a href=\"#Parameterized-mapping-from-images-to-label-scores\" data-toc-modified-id=\"Parameterized-mapping-from-images-to-label-scores-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Parameterized mapping from images to label scores</a></span></li><li><span><a href=\"#Interpreting-a-linear-classifier\" data-toc-modified-id=\"Interpreting-a-linear-classifier-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Interpreting a linear classifier</a></span><ul class=\"toc-item\"><li><span><a href=\"#Analogy-of-images-as-high-dimensional-points.\" data-toc-modified-id=\"Analogy-of-images-as-high-dimensional-points.-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Analogy of images as high-dimensional points.</a></span></li><li><span><a href=\"#Interpretation-of-linear-classifiers-as-template-matching.\" data-toc-modified-id=\"Interpretation-of-linear-classifiers-as-template-matching.-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Interpretation of linear classifiers as template matching.</a></span></li><li><span><a href=\"#Bias-trick.\" data-toc-modified-id=\"Bias-trick.-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Bias trick.</a></span></li><li><span><a href=\"#Image-data-preprocessing.\" data-toc-modified-id=\"Image-data-preprocessing.-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Image data preprocessing.</a></span></li></ul></li><li><span><a href=\"#Loss-function\" data-toc-modified-id=\"Loss-function-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Loss function</a></span><ul class=\"toc-item\"><li><span><a href=\"#Multiclass-Support-Vector-Machine-loss\" data-toc-modified-id=\"Multiclass-Support-Vector-Machine-loss-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Multiclass Support Vector Machine loss</a></span><ul class=\"toc-item\"><li><span><a href=\"#SVM-loss-functions\" data-toc-modified-id=\"SVM-loss-functions-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>SVM loss functions</a></span></li><li><span><a href=\"#Regularization\" data-toc-modified-id=\"Regularization-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Regularization</a></span></li><li><span><a href=\"#Code\" data-toc-modified-id=\"Code-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Code</a></span></li><li><span><a href=\"#Practical-Considerations\" data-toc-modified-id=\"Practical-Considerations-4.1.4\"><span class=\"toc-item-num\">4.1.4&nbsp;&nbsp;</span>Practical Considerations</a></span></li></ul></li><li><span><a href=\"#Softmax-classifier\" data-toc-modified-id=\"Softmax-classifier-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Softmax classifier</a></span><ul class=\"toc-item\"><li><span><a href=\"#Information-theory-view.\" data-toc-modified-id=\"Information-theory-view.-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Information theory view.</a></span></li><li><span><a href=\"#Probabilistic-interpretation.\" data-toc-modified-id=\"Probabilistic-interpretation.-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Probabilistic interpretation.</a></span></li><li><span><a href=\"#Practical-issues:-Numeric-stability.\" data-toc-modified-id=\"Practical-issues:-Numeric-stability.-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Practical issues: Numeric stability.</a></span></li><li><span><a href=\"#Possibly-confusing-naming-conventions-惯例.\" data-toc-modified-id=\"Possibly-confusing-naming-conventions-惯例.-4.2.4\"><span class=\"toc-item-num\">4.2.4&nbsp;&nbsp;</span>Possibly confusing naming conventions 惯例.</a></span></li></ul></li><li><span><a href=\"#SVM-vs.-Softmax\" data-toc-modified-id=\"SVM-vs.-Softmax-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>SVM vs. Softmax</a></span><ul class=\"toc-item\"><li><span><a href=\"#In-practice,-SVM-and-Softmax-are-usually-comparable.\" data-toc-modified-id=\"In-practice,-SVM-and-Softmax-are-usually-comparable.-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>In practice, SVM and Softmax are usually comparable.</a></span></li></ul></li></ul></li><li><span><a href=\"#Interactive-web-demo-of-Linear-Classification\" data-toc-modified-id=\"Interactive-web-demo-of-Linear-Classification-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Interactive web demo of Linear Classification</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Summary</a></span></li><li><span><a href=\"#Further-Reading\" data-toc-modified-id=\"Further-Reading-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Further Reading</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear classification: Support Vector Machine, Softmax\n",
    "---------------------\n",
    "\n",
    "##### INFO\n",
    "* Link: http://cs231n.github.io/linear-classify/\n",
    "\n",
    "## Linear Classification\n",
    "\n",
    "* 上节学习得 KNN 具有明显的缺陷：\n",
    "    * The classifier must remember all of the training data and store it for future comparisons with the test data. This is space inefficient because datasets may easily be gigabytes in size.\n",
    "    * Classifying a test image is expensive since it requires a comparison to all training images.\n",
    "    \n",
    "\n",
    "* [<span class=\"burk\">Overview of this tutorial</span>]:\n",
    "    * going to develop a more powerful approach to image classification that has two major components:\n",
    "        * a **score function** that maps the raw data to class scores\n",
    "        * a **loss function** that quantifies the agreement between the predicted scores and the ground truth labels. \n",
    "    * then cast this as an optimization problem in which we will minimize the loss function with respect to the parameters of the score function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Parameterized mapping from images to label scores\n",
    "* <span class=\"burk\">Score function 的数据模型 $f: R^D \\rightarrow R^K$</span> maps the raw image pixels to class scores. 最简单的是线性模型:\n",
    "$$ f(x_i, W, b) = W x_i + b_i $$\n",
    "原文使用 CIFAR-10 作为例子, 作了说明, 比较简单, 其中 $x_i \\in R^D,\\ y_i \\in R^K$\n",
    "    * $W$ is often called the **weights** or **<span class=\"girk\">parameters</span>**. The single matrix multiplication $Wx_i$ is effectively evaluating 10 separate classifiers in parallel (one for each class), where <span class=\"girk\">each classifier is a row of $W$</span>. Intuitively we wish that the correct class has a score that is higher than the scores of incorrect classes.\n",
    "    * $b_i$ is called the <span class=\"girk\">bias vector</span> because it <span class=\"girk\">influences the output scores, but without interacting with the actual data $x_i$</span>.\n",
    "    * <span class=\"girk\"> Keep the key info/ Effecient Testing.</span> An advantage of this approach is that the training data is used to learn the parameters $W,\\,b$, but <span class=\"girk\">once the learning is complete we can discard the entire training set and only keep the learned</span> parameters. That is because a new test image can be simply forwarded through the function and classified based on the computed scores.\n",
    "    * <span class=\"girk\"> Effective Testing.</span> Lastly, note that to classifying the test image involves a single matrix multiplication and addition, which is significantly faster than comparing a test image to all training images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Interpreting a linear classifier\n",
    "![](http://cs231n.github.io/assets/imagemap.jpg)\n",
    "Figure. 图片的线性多分类器示意图. An example of mapping an image to class scores. we have 3 classes (red (cat), green (dog), blue (ship) class). (Clarification: in particular, the colors here simply indicate 3 classes and are not related to the RGB channels.) Note that this particular set of weights W is not good at all: the weights assign our cat image a very low cat score.\n",
    "\n",
    "### Analogy of images as high-dimensional points.\n",
    "\n",
    "![](http://cs231n.github.io/assets/pixelspace.jpeg)\n",
    "Figure. the images are stretched into high-dimensional column vectors, we can interpret each image as a single point in this space (e.g. each image in CIFAR-10 is a point in 3072-dimensional space of 32x32x3 pixels). Analogously, the entire dataset is a (labeled) set of points.\n",
    "* as we change one of the rows of $W$, the corresponding line in the pixel space will <span class=\"girk\">rotate in different directions</span>.\n",
    "* The biases $b$, on the other hand, allow our classifiers to <span class=\"girk\">translate 平移 the lines</span>.\n",
    "\n",
    "### Interpretation of linear classifiers as template matching. \n",
    "![](http://cs231n.github.io/assets/templates.jpg)\n",
    "Figure. each row of $W$ corresponds to a <span class=\"girk\">template</span> (or sometimes also called a <span class=\"girk\">prototype</span>) for one of the classes.\n",
    "* The score of each class for an image is then obtained by <span class=\"girk\">comparing each template with the image using an inner product (or dot product)</span> one by one to find the one that “fits” best.\n",
    "* Another way to think of it is that <span class=\"girk\">we are still effectively doing Nearest Neighbor</span>, but <span class=\"girk\">instead of having thousands of training images</span> we are <span class=\"girk\">only using a single image per class</span> (although we will learn it, and it does not necessarily have to be one of the images in the training set), and we <span class=\"girk\">use the (negative) inner product as the distance instead of the L1 or L2 distance</span>.\n",
    "\n",
    "\n",
    "有意思的现象:\n",
    "* note that the horse template seems to contain a two-headed horse, which is due to both left and right facing horses in the dataset. The linear classifier merges these two modes of horses in the data into a single template.\n",
    "* Similarly, the car classifier seems to have merged several modes into a single template which has to identify cars from all sides, and of all colors.\n",
    "\n",
    "### Bias trick.\n",
    "![](http://cs231n.github.io/assets/wb.jpeg)\n",
    "Figure. 不就是把 feature 增加一个常量 1, 把 bias 变量吸收到 $W$ 中.\n",
    "\n",
    "### Image data preprocessing.\n",
    "* As a quick note, in the examples above we used the raw pixel values (which range from [0…255]). \n",
    "* In ML, it is a very common to always <span class=\"girk\">perform normalization</span> of your input features. 分为两步:\n",
    "    * <span class=\"girk\">center</span> your data by subtracting the mean from every feature. The new the pixels range from approximately [-127 … 127].\n",
    "    * <span class=\"girk\">scale</span> each input feature so that its values range from [-1, 1].\n",
    "    * of these, <span class=\"girk\">the zero mean centering is arguably more important</span>. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Loss function\n",
    "Intuitively, the loss will be high if we’re doing a poor job of classifying the training data, and it will be low if we’re doing well.\n",
    "\n",
    "<span class=\"burk\">The loss function quantifies our unhappiness with predictions on the training set</span>\n",
    "\n",
    "### Multiclass Support Vector Machine loss\n",
    "\n",
    "#### SVM loss functions \n",
    "* The SVM loss is set up so that the SVM “wants” the correct class for each image to a have a score <span class=\"girk\">higher than the incorrect classes by some fixed margin $\\Delta$</span>.\n",
    "\n",
    "\n",
    "* Suppose the score function is $s_{i}=f\\left(x_{i},W\\right)=Wx_{i}+b_{i}=Wx_{i}$, where $W\\in R^{K\\times D}$. The Multiclass SVM loss for the $i$-th sample is \n",
    "$$L_{i}=\\sum_{j\\neq y_{i}}\\max\\left(0,\\ s_{j}-s_{y_i}+\\Delta\\right)$$\n",
    "where \n",
    "    * $\\Delta$ is a hyperparameter, for example 10.\n",
    "    * $s_{y_i}$ is the score of ture lable's. 我们希望 其他 class 的 score + $\\Delta$ 仍旧比 $s_{y_i}$ 小. 也就是, ground turth class's score is larger than the other class's score by a large margin $\\Delta$.\n",
    "    * 原文中给出了一些例子, 比较简单.\n",
    "    \n",
    "    \n",
    "* 考虑 score function, 我们能够得到新的 objective function as follows:\n",
    "$$ L_{i}=\\sum_{j\\neq y_{i}}\\max\\left(0,\\ w_{j}x_{i}-w_{y_{i}}x_{i}+\\Delta\\right) $$\n",
    "where \n",
    "    * $w_j$ is the $j$-th row of the weight matrix $W\\in R^{K\\times D}$.\n",
    "    * However, this will not necessarily be the case once we start to consider more **complex** forms of the score function $f$.\n",
    "    \n",
    "* Two hinge loss functions:\n",
    "    * [SVM]: <span class=\"girk\">$\\max(0, - )$</span> is often called the <span class=\"girk\">hinge loss 转轴</span>.\n",
    "    * [L2-SVM]: people instead using the <span class=\"girk\">squared hinge loss SVM (or L2-SVM)</span>, which uses the form <span class=\"girk\">$\\max(0, - ) ^ 2$</span> that <span class=\"girk\">penalizes violated margins more strongly</span> (quadratically instead of linearly). \n",
    "    * [Cross-valid to set it]: The <span class=\"girk\">unsquared</span> version is more standard, but in some datasets the squared hinge loss can work better. This can be determined during <span class=\"girk\">cross-validation</span>.\n",
    "    \n",
    "\n",
    "![](http://cs231n.github.io/assets/margin.jpg)\n",
    "Figure. The Multiclass Support Vector Machine \"wants\" the score of the correct class to be higher than all other scores by at least a margin of delta. If any class has a score inside the red region (or higher), then there will be accumulated loss. Otherwise the loss will be zero. Our objective will be to find the weights that will <span class=\"girk\">simultaneously</span> satisfy this constraint for all examples in the training data <span class=\"girk\">and</span> give a total loss that is as low as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization\n",
    "* <span class=\"burk\">Regularization 的必要性</span>,通过一个例子说明:\n",
    "    * [<span class=\"burk\">理由1</span>] 假设 $W$ 能够使得分类全部准确且 loss 完全等于0, 那么很多参数 $\\lambda W$  where $\\lambda\\geq 1$ 都能满足条件. <span class=\"girk\">For example</span>, if the difference in scores between a correct class and a nearest incorrect class was 15, then multiplying all elements of W by 2 would make the new difference 30.\n",
    "    * [<span class=\"burk\">理由1</span>: <span class=\"girk\">去除歧义</span>] In other words, we wish to encode some preference for a certain set of weights W over others to remove this ambiguity.\n",
    "    * [<span class=\"burk\">理由2</span>] <span class=\"girk\">including the L2 penalty</span> leads to the appealing max margin property in SVMs (See [CS229](http://cs229.stanford.edu/notes/cs229-notes3.pdf) lecture notes for full details if you are interested).\n",
    "    * [<span class=\"burk\">理由3</span>] The most appealing property is that penalizing large weights tends to <span class=\"girk\">improve generalization</span>, because it means that no input dimension can have a very large influence on the scores all by itself. \n",
    "    * [<span class=\"burk\">L2 regularization 分析</span>] Since the L2 penalty prefers <span class=\"girk\">smaller and more diffuse 弥漫漫射 weight vectors</span>, the final classifier is encouraged to take into account all input dimensions to small amounts rather than a few input dimensions and very strongly. As we will see later in the class, this effect can improve the generalization performance of the classifiers on test images and lead to less overfitting.\n",
    "    * [<span class=\"burk\">对 bias 正则的讨论</span>] Note that biases do not have the same effect since, unlike the weights, they do not control the strength of influence of an input dimension. Therefore, it is common to only regularize the weights $W$ but not the biases $b$. However, in practice this often turns out to have a <span class=\"girk\">negligible 微不足道的 effect</span>.\n",
    "    * [有了 regularization, loss function 基本不可能为 0]\n",
    "\n",
    "\n",
    "\n",
    "* 最常见的 regularization penalty $R(W)$ 是基于 L2 norm. It <span class=\"girk\">discourages large weights</span> through an elementwise quadratic penalty over all parameters:\n",
    "$$R(W) = \\sum_i\\sum_j W_{ij}^2$$\n",
    "\n",
    "\n",
    "* SVM 的综合 loss function 包含两个部分: (1) the <span class=\"girk\">data loss</span> (which is the average loss $L_i$ over all examples) and (2) the <span class=\"girk\">regularization loss</span>:\n",
    "$$\n",
    "    L = \\frac{1}{N} \\sum_i L_i + \\lambda \\sum_i\\sum_j W_{ij}^2\n",
    "$$\n",
    "展开来写\n",
    "\\begin{align*}\n",
    "L & =\\frac{1}{N}\\sum_{i=1}^{N}\\left[\\sum_{j\\neq y_{i}}\\max\\left(0,\\ f\\left(x_{i},w_{j}\\right)-f\\left(x_{i},w_{y_{i}}\\right)+\\Delta\\right)\\right]+\\lambda\\sum_{i}\\sum_{j}W_{ij}^{2}\\\\\n",
    " & =\\frac{1}{N}\\sum_{i=1}^{N}\\left[\\sum_{j\\neq y_{i}}\\max\\left(0,\\ f\\left(x_{i},W\\right)_{j}-f\\left(x_{i},W\\right)_{y_{i}}+\\Delta\\right)\\right]+\\lambda\\sum_{i}\\sum_{j}W_{ij}^{2}\n",
    "\\end{align*}\n",
    "where $\\lambda$ is a hyperparameter. There is no simple way of setting this hyperparameter and it is usually determined by cross-validation.\n",
    "\n",
    "  \n",
    "#### Code \n",
    "Here is the loss function (<span class=\"girk\">without regularization</span>) implemented in Python, in both <span class=\"girk\">unvectorized (两重 for 循环)</span> and <span class=\"girk\">half-vectorized (一重 for 循环) </span> and <span class=\"girk\">fully-vectorized (0重 for 循环) </span> form. \n",
    "\n",
    "剩下的事情是如何优化得到 weights $W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_i(x, y, W):\n",
    "    \"\"\"\n",
    "    unvectorized version. Compute the multiclass svm loss for a single example (x,y)\n",
    "    - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)\n",
    "    with an appended bias dimension in the 3073-rd position (i.e. bias trick)\n",
    "    - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)\n",
    "    - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)\n",
    "    \"\"\"\n",
    "    delta = 1.0 # see notes about delta later in this section\n",
    "\n",
    "    # scores becomes of size 10 x 1, the scores for each class\n",
    "    scores = W.dot(x) \n",
    "    correct_class_score = scores[y]\n",
    "    D = W.shape[0] # number of classes, e.g. 10\n",
    "    loss_i = 0.0\n",
    "    for j in xrange(D): # iterate over all wrong classes\n",
    "        if j == y:\n",
    "            # skip for the true class to only loop over incorrect classes\n",
    "            continue\n",
    "        # accumulate loss for the i-th example\n",
    "        loss_i += max(0, scores[j] - correct_class_score + delta)\n",
    "    return loss_i\n",
    "\n",
    "def L_i_vectorized(x, y, W):\n",
    "    \"\"\"\n",
    "    A faster half-vectorized implementation. half-vectorized\n",
    "    refers to the fact that for a single example the implementation contains\n",
    "    no for loops, but there is still one loop over the examples (outside this function)\n",
    "    \"\"\"\n",
    "    delta = 1.0\n",
    "    scores = W.dot(x)\n",
    "    # compute the margins for all classes in one vector operation\n",
    "    margins = np.maximum(0, scores - scores[y] + delta)\n",
    "    # on y-th position scores[y] - scores[y] canceled and gave delta. We want\n",
    "    # to ignore the y-th position and only consider margin on max wrong class\n",
    "    margins[y] = 0\n",
    "    loss_i = np.sum(margins)\n",
    "    return loss_i\n",
    "\n",
    "def L(X, y, W):\n",
    "    \"\"\"\n",
    "    fully-vectorized implementation :\n",
    "    - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)\n",
    "    - y is array of integers specifying correct class (e.g. 50,000-D array)\n",
    "    - W are weights (e.g. 10 x 3073)\n",
    "    \"\"\"\n",
    "    # evaluate loss over all examples in X without using any for loops\n",
    "    # left as exercise to reader in the assignment\n",
    "    delta = 1.0\n",
    "    X     = np.transpose (X)\n",
    "    N     = X.shape[0]\n",
    "    \n",
    "    Y_hat   = np.dot (X, np.transpose(W) )  # W: K x D, X: N x D, Y_hat: N x K\n",
    "    margins = np.maximum (0, Y_hat - Y_hat (np.nrange(N), y ) + delta) \n",
    "    margins(np.nrange(N), y) = 0\n",
    "    loss    = np.mean (margins, axis = 1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Considerations\n",
    "##### Setting Delta.\n",
    "* Two hyperparameters $\\Delta$ and the balancing parameter $\\lambda$. \n",
    "    * cross-validate? \n",
    "    * It turns out that this hyperparameter can safely be set to $\\Delta=1.0$ in all cases.\n",
    "    \n",
    "    \n",
    "\n",
    "* The hyperparameters $\\Delta$ and $\\lambda$ seem like two different hyperparameters, but in fact they <span class=\"girk\">both control</span> <span class=\"girk\">the same tradeoff</span>: The <span class=\"girk\">tradeoff between</span> the data loss <span class=\"girk\">and</span> the regularization loss in the objective. \n",
    "    * [<span class=\"girk\">$\\lambda$ 比较容易理解</span>] 它就是为了调节 data loss 和 regularization 的权重.\n",
    "    * [<span class=\"girk\">理解的关键是 $\\Delta$ 是怎么调节的?</span>] the magnitude of the weights $W$ has direct effect on the scores (and hence also their differences): \n",
    "        * [<span class=\"girk\">小的$\\Delta$ 更重视 regularization: loss 小了, regularization 就重视了</span>] as we shrink all values in $W$ the **score differences** will become lower\n",
    "        * [<span class=\"girk\">大的$\\Delta$ 更重视  data loss: loss大了, regularization就不被重视了</span>] as we scale up the weights the score differences will all become higher.\n",
    "       \n",
    "       \n",
    "* 调整超参的方法:\n",
    "    * Therefore, the exact value of the margin between the scores (e.g. $\\Delta=1$ or $\\Delta=100$) is <span class=\"girk\">in some sense meaningless</span> because the weights can shrink or stretch the differences arbitrarily. \n",
    "    * Hence, **the only real tradeoff is how large we allow the weights to grow (through the regularization strength $\\lambda$)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Relation to Binary Support Vector Machine (binary class 两类问题)\n",
    "\n",
    "Loss function\n",
    "$$ L_i = C \\max(0, 1 - y_i W^\\top x_i) + R(W) $$\n",
    "where $y_i\\in\\{-1, 1\\}$, 和 multi class SVM 对比:\n",
    "* 当 $W^\\top x_i$ 和 $y_i$ 同号, 则相减. 如果 $ \\left|W^{\\top}x_{i}\\right| \\geq 1$, 则loss 忽略不计; 否则, margin 不够大.\n",
    "* 当 $W^\\top x_i$ 和 $y_i$ 异号, 则相加, loss 更大. \n",
    "* 这里 hyper-parameter 是 $C$, 没有 $\\lambda$, 其实可以理解为 $C\\sim \\frac{1}{\\lambda}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aside 旁白: Optimization in primal 主要. \n",
    "* SVM 还涉及几个重要概念: kernels, duals, the SMO algorithm, etc\n",
    "* Optimization: \n",
    "    * Many of these objectives are technically <span class=\"girk\">not differentiable</span> (e.g. the max(x,y) function isn’t because it has a <span class=\"girk\">kink</span> when x=y),\n",
    "    * but in practice this is <span class=\"girk\">not a problem</span> and it is common to <span class=\"girk\">use a subgradient</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aside 旁白: Other Multiclass SVM formulations (OVA, AVA)\n",
    "* Two multiclass SVM formulations:\n",
    "    * [<span class=\"burk\">All-vs-All: AVA</span>] It is worth noting that the Multiclass SVM presented in this section is one of few ways of formulating the SVM over multiple classes. \n",
    "    * [<span class=\"burk\">One-Vs-All: OVA</span>] Another commonly used form is the <span class=\"girk\">One-Vs-All (OVA) SVM</span> which <span class=\"girk\">trains an independent binary SVM for each class vs. all other classes</span>. \n",
    "    * [<span class=\"burk\">Structured SVM</span>] it maximizes the margin between the score of the correct class and the score of the highest-scoring incorrect runner-up class. \n",
    "    \n",
    "    \n",
    "* The comparasion between those two SVM formulations:\n",
    "    * Our formulation follows the [Weston and Watkins 1999 (pdf)](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es1999-461.pdf) version, which is a more powerful version than OVA (in the sense that you can construct multiclass datasets where this version can achieve zero data loss, but OVA cannot. See details in the paper if interested)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax classifier\n",
    "\n",
    "* Softmax classifer's backgrounds:\n",
    "    * Two most popular classifiers: SVM and softmax.\n",
    "    * Softmax is an extension to multiple classes. of the Logistic Regression classifier \n",
    "    \n",
    "    \n",
    "* Softmax 与 SVM 对比:\n",
    "    * Unlike the SVM which treats the outputs $f(x_i,W)$ as (uncalibrated 未校准 and possibly difficult to interpret) scores for each class, the Softmax classifier gives a <span class=\"girk\">slightly more intuitive output</span> (<span class=\"girk\">normalized class probabilities</span>) and also has a <span class=\"girk\">probabilistic interpretation</span>. \n",
    "    * In the Softmax classifier, the function <span class=\"girk\">mapping $f(x_i; W) =  W x_i$ stays unchanged</span>. We replace the <span class=\"girk\">hinge loss</span> with a <span class=\"girk\">cross-entropy loss</span>.\n",
    "    \n",
    "    \n",
    "* The <span class=\"girk\">cross-entropy loss</span> for the softmax classifier: $$ L_i = -\\log\\left(\\frac{\\exp(f_{y_i})}{ \\sum_j \\exp({f_j}) }\\right) \\hspace{0.3in} \\text{or equivalently} \\hspace{0.3in} L_i = -f_{y_i} + \\log\\sum_j \\exp({f_j}) $$ \n",
    " where\n",
    "    * ${\\displaystyle f_{j}=\\frac{\\exp\\left(z_{j}\\right)}{\\sum_{k=1}^{K}\\exp\\left(z_{k}\\right)}}$ is the $j$-th element of the vector of class scores $f$. \n",
    "    * The defintion of $f_j$ is softmax function. It takes a vector of arbitrary real-valued scores (in $z$) and squashes 挤进 it to a vector of values between zero and one that sum to one.\n",
    "    *  As before, the full loss for the dataset is <span class=\"girk\">the mean of $L_i$</span> over all training examples <span class=\"mark\">together with</span> a regularization term $R(W)$.\n",
    "    \n",
    "* 考虑regularization term, 如果 regularization 太强, the output probabilities would be near uniform. 因此, softmax's\n",
    "probability 和 SVM's score 类似, 更多地是表示一种 confidence 承担."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information theory view.\n",
    "* The cross-entropy between a <span class=\"girk\">“true” distribution $p$</span> and an <span class=\"girk\">estimated distribution $q$</span> is defined as:\n",
    "$$ H(p,q) = - \\sum_x p(x) \\log q(x) $$\n",
    "\n",
    "\n",
    "* The Softmax classifier is hence **<span class=\"burk\">minimizing the cross-entropy</span>** <span class=\"mark\">between</span> the estimated class probabilities ($q = \\exp({f_{y_i}})  / \\sum_j \\exp(f_j)$ as seen above) <span class=\"mark\">and</span> the “true” distribution of correct class lables of each sample. \n",
    "    * 因为 $p$ 说是分布, 它其实是确定值 ($p=[0,\\cdots,1,\\cdots,0]$, 只有一个元素为 1, 其他都为 0), 没有概率意义. 因此使得 cross-entropy 定义中的 $\\sum$ 和 $p$ 都去除了.\n",
    "\n",
    "\n",
    "* Moreover, since the cross-entropy can be written in terms of entropy and the Kullback-Leibler divergence as $H(p,q) = H(p) + D_{KL}(p||q)$\n",
    "    * the entropy of the delta function $p$ is zero (太确定了, 所以熵为 0),\n",
    "    * this is also equivalent to minimizing the <span class=\"girk\">KL divergence</span> between the two distributions (a <span class=\"girk\">measure of distance</span>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilistic interpretation.\n",
    "The the expression\n",
    "$$ P(y_i \\mid x_i; W) = \\frac{\\exp({f_{y_i}}) }{\\sum_j \\exp({f_j}) } $$\n",
    "can be interpreted as the <span class=\"girk\">(normalized) probability</span> **assigned to** the correct label $y_i$ given the image $x_i$ and parameterized by $W$.\n",
    "* the Softmax classifier interprets the <span class=\"girk\">scores</span> inside the output vector $f$ as the <span class=\"girk\">unnormalized log probabilities</span>.\n",
    "* In the probabilistic interpretation, we are therefore <span class=\"girk\">minimizing the negative log likelihood</span> of the correct class, which can be interpreted as <span class=\"girk\">performing Maximum Likelihood Estimation (MLE)</span>.\n",
    "* We can now also interpret the regularization term $R(W)$ in the full loss function as coming from a <span class=\"girk\">Gaussian prior</span> over the weight matrix $W$, where <span class=\"girk\">instead of MLE</span> we are <span class=\"girk\">performing the Maximum a posteriori (MAP) estimation</span>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical issues: Numeric stability.\n",
    "根据 $P(y_i \\mid x_i; W)$ 的定义, 其中 $\\exp({f_{y_i}})$ 和 $\\sum_j \\exp({f_j})$ may be very large due to the exponentials.\n",
    "* 为了解决上述问题, we <span class=\"girk\">multiply the top and bottom</span> of the fraction by a constant $C$ and <span class=\"girk\">push it into the sum</span>, we get the following (mathematically equivalent) expression:\n",
    "$$\n",
    "    \\frac{\\exp\\left(f_{y_{i}}\\right)}{\\sum_{j}\\exp\\left(f_{j}\\right)}=\\frac{C\\exp\\left(f_{y_{i}}\\right)}{C\\sum_{j}\\exp\\left(f_{j}\\right)}=\\frac{\\exp\\left(f_{y_{i}}+\\log C\\right)}{\\sum_{j}\\exp\\left(f_{j}+\\log C\\right)}\n",
    "$$\n",
    "* <span class=\"girk\">$C$ is free to choose. It will not change any of the results</span>, but we can use this value to <span class=\"girk\">improve the numerical stability</span> of the computation. A <span class=\"girk\">common choice</span> for $C$ is to set $\\log C=−\\max_jf_j$, 其实是$f$ 中最大的数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "f = np.array([123, 456, 789]) # example with 3 classes and each having large scores\n",
    "p = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup\n",
    "\n",
    "# instead: first shift the values of f so that the highest number is 0:\n",
    "f -= np.max(f) # f becomes [-666, -333, 0]\n",
    "p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Possibly confusing naming conventions 惯例.\n",
    "* SVM classifier: hinge loss OR max-margin loss.\n",
    "* Softmax classifier: <span class=\"girk\">cross-entropy loss</span>. Technically it doesn’t make sense to talk about the “softmax loss”, since softmax is just the squashing function, but it is a relatively commonly used shorthand 速记."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM vs. Softmax\n",
    "![](http://cs231n.github.io/assets/svmvssoftmax.png)\n",
    "Figure. SVM vs. Softmax. In both cases we compute the same score vector f (e.g. by matrix multiplication in this section). The difference is in the interpretation of the scores in f: \n",
    "* SVM interprets these as class scores and its loss function encourages the correct class (class 2, in blue) to have a score higher by a margin than the other class scores\n",
    "* Softmax classifier instead interprets the scores as <span class=\"girk\">(unnormalized) log probabilities</span> for each class and then encourages the (normalized) log probability of the correct class to be high (equivalently the negative of it to be low). \n",
    "$$[1, -2, 0] \\rightarrow [e^1, e^{-2}, e^0] = [2.71, 0.14, 1] \\rightarrow [0.7, 0.04, 0.26]$$\n",
    "\n",
    "SVM loss is 1.58, softmax loss is 1.04. These numbers are not comparable; They are only meaningful in relation to loss computed within the same classifier and with the same data.\n",
    "\n",
    "#### In practice, SVM and Softmax are usually comparable. \n",
    "* The performance difference between the SVM and Softmax are usually very small, and different people will have different opinions on which classifier works better.\n",
    "* Compared to the Softmax classifier, the <span class=\"girk\">SVM is a **more local objective</span>**, which could be thought of either as a bug or a feature.\n",
    "    * [10, -100, -100] or [10, 9, 9] to the SVM would be indifferent since the margin of 1 is satisfied and hence the loss is zero.\n",
    "    * However, these scenarios are not equivalent to a Softmax classifier, which would accumulate a much higher loss for the scores [10, 9, 9] than for [10, -100, -100].\n",
    "\n",
    "\n",
    "* In other words,\n",
    "    * [<span class=\"girk\">关注所有</span>] the Softmax classifier is never fully happy with the scores it produces: the correct class could always have a higher probability and the incorrect classes always a lower probability and the loss would always get better. \n",
    "    * [<span class=\"girk\">只关注最主要</span>] However, the SVM is happy once the margins are satisfied and it does not micromanage the exact scores beyond this constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Interactive web demo of Linear Classification\n",
    "\n",
    "[Link to the interesting web demo](http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "## Summary\n",
    "* We defined a <span class=\"mark\">score function</span> from image pixels to class scores (in this section, a linear function that depends on weights W and biases b).\n",
    "* Unlike kNN classifier, the advantage of this <span class=\"burk\"><span class=\"mark\">parametric approach</span></span> is that once we learn the parameters we can discard the training data. Additionally, the prediction for a new test image is fast since it requires a single matrix multiplication with W, not an exhaustive comparison to every single training example.\n",
    "* We introduced the <span class=\"mark\">bias trick</span>, which allows us to fold the bias vector into the weight matrix for convenience of only having to keep track of one parameter matrix.\n",
    "* We defined a <span class=\"mark\">loss function</span> (we introduced two commonly used losses for linear classifiers: the SVM and the Softmax) that measures how compatible a given set of parameters is with respect to the ground truth labels in the training dataset. We also saw that the loss function was defined in such way that making good predictions on the training data is equivalent to having a small loss.\n",
    "* not touched how to optimization yet!</div><i class=\"fa fa-lightbulb-o \"></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Further Reading\n",
    "[Deep Learning using Linear Support Vector Machines](http://arxiv.org/abs/1306.0239)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": true,
   "toc_position": {
    "height": "550px",
    "left": "0px",
    "right": "867.2px",
    "top": "74px",
    "width": "151px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
