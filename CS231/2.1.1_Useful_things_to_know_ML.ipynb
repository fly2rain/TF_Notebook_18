{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Learning-=-representation-+-evaluation-+-optimization\" data-toc-modified-id=\"Learning-=-representation-+-evaluation-+-optimization-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Learning = representation + evaluation + optimization</a></span></li><li><span><a href=\"#It's-generalization-that-counts\" data-toc-modified-id=\"It's-generalization-that-counts-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>It's generalization that counts</a></span></li><li><span><a href=\"#Data-alone-is-not-enough\" data-toc-modified-id=\"Data-alone-is-not-enough-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data alone is not enough</a></span></li><li><span><a href=\"#Intuition-fails-in-high-dimensions\" data-toc-modified-id=\"Intuition-fails-in-high-dimensions-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Intuition fails in high dimensions</a></span></li><li><span><a href=\"#Theoretical-guarantees-are-not-what-they-seem\" data-toc-modified-id=\"Theoretical-guarantees-are-not-what-they-seem-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Theoretical guarantees are not what they seem</a></span></li><li><span><a href=\"#Feature-engineering-is-the-key\" data-toc-modified-id=\"Feature-engineering-is-the-key-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Feature engineering is the key</a></span></li><li><span><a href=\"#More-data-beats-a-cleverer-algorithm\" data-toc-modified-id=\"More-data-beats-a-cleverer-algorithm-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>More data beats a cleverer algorithm</a></span></li><li><span><a href=\"#learn-many-models,-not-just-one\" data-toc-modified-id=\"learn-many-models,-not-just-one-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>learn many models, not just one</a></span></li><li><span><a href=\"#simplicity-does-not-imply-accuracy\" data-toc-modified-id=\"simplicity-does-not-imply-accuracy-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>simplicity does not imply accuracy</a></span></li><li><span><a href=\"#Representable-does-not-imply-learnable\" data-toc-modified-id=\"Representable-does-not-imply-learnable-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Representable does not imply learnable</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Few Useful Things to Know about Machine Learning\n",
    "##### INFO\n",
    "* Pedro Domingos\n",
    "\n",
    "###### Abstract\n",
    "* **ML 原理、优势**：perform important tasks by **generalizing from examples**. **优势: feasible and cost-effective** where manual programming is not.\n",
    "* **ML 潜力**：As more data becomes available, more ambitious problems can be tackled. 因此，ML 广泛应用于 CS  and other fields.\n",
    "* **问题**：developing successful machine learning applications requires a **substantial amount of “black art”** that is hard to find in textbooks.\n",
    "* **解决**：This article **summarizes twelve key lessons** that machine learning researchers and practitioners have learned. Including：(1) pitfalls 陷阱 to avoid, (2) important issues to focus on, (3) answers to common questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "* Machine learning systems **automatically learn programs from data**. \n",
    "    * This is often a very attractive alternative to manually constructing them, and in the last decade the use of machine learning has spread rapidly throughout computer science and beyond.\n",
    "    * Machine learning is used in Web search, spam 垃圾邮件 filters, recommender systems, ad placement 放置, credit scoring, fraud 诈骗 detection, stock trading, drug design, and many other applications. \n",
    "    * machine learning (a.k.a. data mining or predictive analytics) will be the driver of the next big wave of innovation [16].\n",
    "    * much of this folk knowledge 民间知识 is fairly easy to communicate. This is the purpose of this article\n",
    "* 本文将以 classification 为例介绍 skills，因为它是最成熟的 ML算法，但适用于所有 ML.\n",
    "* spam filters 建模 (参照 introduction 第2段，有意思)\n",
    "    * 原来 feature 是类似于 bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning = representation + evaluation + optimization\n",
    "> ML 算法成千上万，为了简单梳理，它们主要包含以下几个模块：\n",
    "\n",
    "> **Representation**. A classifier must be represented in some formal language that the computer can handle.\n",
    "* [**ML alg**] This set is called the hypothesis space of the learner. If a classifier is not in the hypothesis space, it cannot be learned. \n",
    "* [**Feature**] A related question, which we will address in a later section, is how to represent the input, i.e., what features to use.\n",
    "    \n",
    "> **Evaluation**. \n",
    "* An evaluation function (also called *objective function or scoring 得分 function*) is needed to distinguish good classifiers from bad ones.\n",
    "* [是这样的，以前碰到过] The evaluation function used internally 内部地 by the algorithm may differ from the external 外部的 one that we want the classifier to optimize,\n",
    "    * for ease of optimization (see below) \n",
    "    * and due to the issues discussed in the next section.\n",
    "\n",
    "> **Optimization**.\n",
    "* The choice of optimization technique is key to the efficiency of the learner, and also helps determine the classifier produced if the evaluation function has more than one optimum. \n",
    "* It is common for new learners to start out using off-the-shelf optimizers, which are later replaced by custom-designed ones.\n",
    "    \n",
    "--------  \n",
    "> Table 1 shows common examples of each of these three com- ponents. \n",
    "![](http://api.ning.com/files/MOC7BqAcxPuQS9W2NCsCd*jZz7jTP39e9bqmOSqFihVTYgAlO4ZO0y1pQVvzW*bCauKaHkeFopxC0mdApwZzBVeyGHfEqB0x/Figure1_ML_Components.png)\n",
    "- [**<span class=\"mark\">instances based</span>**] KNN classifies a test example by finding the $k$ most similar training examples and predicting the majority class among them\n",
    "- [**<span class=\"mark\">Hyperplane based</span>**] form a linear combination of the features per\n",
    "class and predict the class with the highest-valued combination.\n",
    "- [**<span class=\"girk\"><span class=\"mark\">Decision trees</span></span>**] test one feature at each internal 内部的 node, with one branch for **each feature value**, and **have class predictions at the leaves**. Algorithm 1 shows a bare-bones decision tree learner for Boolean domains:\n",
    "\n",
    "\n",
    "> not all combinations of one component from each column of Table make equal sense.\n",
    "- discrete representations naturally go with combinatorial optimization, and continuous ones with continuous optimization.\n",
    "- Most textbooks are organized by representation, and it’s easy to overlook the fact that the other components are equally important. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's generalization that counts\n",
    "* The fundamental goal of machine learning is to <span class=\"mark\">generalize beyond the examples in the training set</span>.\n",
    "    * This is because, no matter how much data we have, it is very unlikely that we will see those exact examples again at test time. \n",
    "    * Notice that, if there are 100,000 words in the dictionary, the spam filter described above has $2^{100,000}$ possible different inputs.) \n",
    "    * Doing well on the training set is easy (just memorize the examples).\n",
    "    \n",
    "* The <span class=\"mark\">most common mistake among machine learning beginners</span> is to test on the training data and have the illusion 错觉 of success.\n",
    "    * If the chosen classifier is then tested on new data, it is often no better than random guessing.\n",
    "    * So, if you hire someone to build a classifier, be sure to keep some of the data to yourself and test the classifier they give you on it.\n",
    "    * Conversely, if you’ve been hired to build a classifier, set some of the data aside from the beginning, and only use it to test your chosen classifier at the very end, followed by learning your final classifier on the whole data.\n",
    "    \n",
    "* In the early days of machine learning, the need to keep training and test data separate was not widely appreciated. [模型复杂度本来就不高，不易过拟合 overfitting]\n",
    "    * This was partly because, if the learner has a very limited representation (e.g., hyperplanes), the difference between training and test error may not be large. \n",
    "    * But with very <span class=\"mark\">flexible classifiers (e.g., decision trees)</span>, or even with linear classifiers with a lot of features, strict separation is mandatory.\n",
    "    \n",
    "* Notice that <span class=\"mark\">generalization being the goal</span> has an interesting consequence for machine learning. 【testing error 最重要，但没法获得，只能用training err近似】\n",
    "    * Unlike in most other optimization problems, we <span class=\"mark\">don’t have access to the function we want to optimize</span>! We have to <span class=\"mark\">use training error as a surrogate 代理 for test error</span>, and this is <span class=\"mark\">fraught with danger</span>。\n",
    "    * (有意思的观点) On the positive side, since the objective function is only a proxy for the true goal, we may not need to fully optimize it; in fact, a local optimum returned by simple greedy search may be better than the global optimum.\n",
    "    \n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data alone is not enough\n",
    "Generalization being the goal has another major consequence:\n",
    "* data alone is not enough, no matter how much of it you have.\n",
    "    * Consider learning a Boolean function of (say) 100 variables from a million examples. There are $2^{100} − 10^6$ examples whose classes you don’t know. How do you figure out what those classes are?\n",
    "    * In the absence of further information, there is just no way to do this that beats flipping a coin.\n",
    "    * Every learner must embody 包含 <span class=\"mark\">some knowledge or assumptions</span> beyond the data it’s given in order to generalize beyond it.\n",
    "    \n",
    "* Luckily, the functions we want to learn in the real world are not drawn uniformly from the set of <span class=\"mark\">all mathematically possible functions</span>!\n",
    "    * very general assumptions—like smoothness, <span class=\"mark\">similar examples having similar classes</span>, <span class=\"mark\">limited dependences</span>, or <span class=\"mark\">limited complexity</span>—are often enough to do very well,\n",
    "    * and this is a large part of why machine learning has been so successful.\n",
    "    \n",
    "* the key criteria for choosing a representation is which kinds of knowledge are easily expressed in it. \n",
    "    * if we have a lot of knowledge about what makes examples similar in our domain, instancebased methods may be a good choice.\n",
    "    * If we have knowledge about probabilistic dependencies, graphical models are a good fit. \n",
    "    * And if we have knowledge about what kinds of preconditions 前提 are required by each class, “IF . . . THEN . . .” rules may be the the best option.\n",
    "    \n",
    "* The most useful learners in this regard are those that don’t just have assumptions hard-wired 硬接线 into them, but allow us to state them explicitly, vary them widely, and incorporate them automatically intothe learning (e.g., using first-order logic [22] or grammars [6]).\n",
    "\n",
    "* In retrospect 回顾, the need for knowledge in learning should not be surprising. Machine learning is not magic; it can’t get something from nothing. What it does is get more from\n",
    "less. \n",
    "    * Programming, like all engineering, is a lot of work: we have to build everything from scratch.\n",
    "    * Learning is more like farming, which lets nature do most of the work. \n",
    "        * Farmers combine seeds with nutrients to grow crops.\n",
    "        * Learners combine knowledge with data to grow programs.\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition fails in high dimensions\n",
    "*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical guarantees are not what they seem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering is the key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More data beats a cleverer algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learn many models, not just one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simplicity does not imply accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representable does not imply learnable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": true,
   "toc_position": {
    "height": "535px",
    "left": "0px",
    "right": "20px",
    "top": "106px",
    "width": "149px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
