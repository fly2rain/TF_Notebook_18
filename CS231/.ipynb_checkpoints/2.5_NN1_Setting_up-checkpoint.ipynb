{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Quick-intro\" data-toc-modified-id=\"Quick-intro-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Quick intro</a></span></li><li><span><a href=\"#Biological-motivation-and-connections\" data-toc-modified-id=\"Biological-motivation-and-connections-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Biological motivation and connections</a></span></li><li><span><a href=\"#Single-neuron-as-a-linear-classifier\" data-toc-modified-id=\"Single-neuron-as-a-linear-classifier-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Single neuron as a linear classifier</a></span><ul class=\"toc-item\"><li><span><a href=\"#Binary-Softmax-classifier.\" data-toc-modified-id=\"Binary-Softmax-classifier.-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Binary Softmax classifier.</a></span></li><li><span><a href=\"#Binary-SVM-classifier\" data-toc-modified-id=\"Binary-SVM-classifier-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Binary SVM classifier</a></span></li><li><span><a href=\"#Regularization-interpretation\" data-toc-modified-id=\"Regularization-interpretation-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Regularization interpretation</a></span></li></ul></li><li><span><a href=\"#Commonly-used-activation-functions\" data-toc-modified-id=\"Commonly-used-activation-functions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Commonly used activation functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sigmoid\" data-toc-modified-id=\"Sigmoid-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Sigmoid</a></span><ul class=\"toc-item\"><li><span><a href=\"#对-sigmoid-的分析：\" data-toc-modified-id=\"对-sigmoid-的分析：-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>对 sigmoid 的分析：</a></span></li></ul></li><li><span><a href=\"#Tanh\" data-toc-modified-id=\"Tanh-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Tanh</a></span></li><li><span><a href=\"#ReLU\" data-toc-modified-id=\"ReLU-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>ReLU</a></span></li><li><span><a href=\"#Leaky-ReLU\" data-toc-modified-id=\"Leaky-ReLU-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Leaky ReLU</a></span></li><li><span><a href=\"#Maxout\" data-toc-modified-id=\"Maxout-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Maxout</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></li><li><span><a href=\"#Neural-Network-architectures\" data-toc-modified-id=\"Neural-Network-architectures-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Neural Network architectures</a></span><ul class=\"toc-item\"><li><span><a href=\"#Layer-wise-organization\" data-toc-modified-id=\"Layer-wise-organization-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Layer-wise organization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Neural-Networks-as-neurons-in-graphs\" data-toc-modified-id=\"Neural-Networks-as-neurons-in-graphs-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Neural Networks as neurons in graphs</a></span></li><li><span><a href=\"#Naming-conventions\" data-toc-modified-id=\"Naming-conventions-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Naming conventions</a></span></li><li><span><a href=\"#Output-layer\" data-toc-modified-id=\"Output-layer-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Output layer</a></span></li><li><span><a href=\"#Sizing-neural-networks.\" data-toc-modified-id=\"Sizing-neural-networks.-5.1.4\"><span class=\"toc-item-num\">5.1.4&nbsp;&nbsp;</span>Sizing neural networks.</a></span></li></ul></li><li><span><a href=\"#Example-feed-forward-computation\" data-toc-modified-id=\"Example-feed-forward-computation-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Example feed-forward computation</a></span></li><li><span><a href=\"#Representational-power\" data-toc-modified-id=\"Representational-power-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Representational power</a></span></li><li><span><a href=\"#Setting-number-of-layers-and-their-sizes\" data-toc-modified-id=\"Setting-number-of-layers-and-their-sizes-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Setting number of layers and their sizes</a></span><ul class=\"toc-item\"><li><span><a href=\"#How-do-we-decide-on-what-architecture-to-use-when-faced-with-a-practical-problem?\" data-toc-modified-id=\"How-do-we-decide-on-what-architecture-to-use-when-faced-with-a-practical-problem?-5.4.1\"><span class=\"toc-item-num\">5.4.1&nbsp;&nbsp;</span>How do we decide on what architecture to use when faced with a practical problem?</a></span></li><li><span><a href=\"#Large-NN-&gt;-Small-NN-Reasons\" data-toc-modified-id=\"Large-NN->-Small-NN-Reasons-5.4.2\"><span class=\"toc-item-num\">5.4.2&nbsp;&nbsp;</span>Large NN &gt; Small NN Reasons</a></span></li><li><span><a href=\"#Large-NN-with-L2-regularization\" data-toc-modified-id=\"Large-NN-with-L2-regularization-5.4.3\"><span class=\"toc-item-num\">5.4.3&nbsp;&nbsp;</span>Large NN with L2 regularization</a></span></li><li><span><a href=\"#Takeaway-notes\" data-toc-modified-id=\"Takeaway-notes-5.4.4\"><span class=\"toc-item-num\">5.4.4&nbsp;&nbsp;</span>Takeaway notes</a></span></li></ul></li></ul></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Summary</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# [Neural Networks Part 1: Setting up the Architecture](http://cs231n.github.io/neural-networks-1/)\n",
    "\n",
    "model of a biological neuron, activation functions, neural net architecture, representational power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick intro\n",
    "* <span class=\"burk\">Two layer neural network's</span> score function:\n",
    "$$ s = W_2 \\max(0, W_1 x) $$\n",
    "where \n",
    "    * $W_1 \\in R^{100 \\times 3072}$ and $W_2 \\in R^{10\\times 100}$ parameters or weights\n",
    "    * $\\max(0, -)$ is a non-linearity that is applied elementwise.\n",
    "    \n",
    "    \n",
    "* <span class=\"burk\">Three layer  neural network</span> score function:\n",
    "$$ s = W_3 \\max(0, W_2 \\max(0, W_1 x)) $$\n",
    "where \n",
    "    * all of $W_3,W_2,W_1$ are parameters to be learned.\n",
    "    * The sizes of the <span class=\"girk\">intermediate hidden vectors are hyperparameters</span> of the network and we’ll see how we can set them later. \n",
    "    \n",
    "    \n",
    "## Biological motivation and connections\n",
    "<img src=\"http://cs231n.github.io/assets/nn1/neuron_model.jpeg\" width=\"425\"/>\n",
    "\n",
    "* Based on this rate code interpretation, we model the firing rate of the neuron with an activation function $f$, which represents the frequency of the spikes along the axon. \n",
    "* Historically, a common choice of activation function is the sigmoid function ${\\displaystyle \\sigma=\\frac{1}{1+\\exp(-x)}}$, since it takes a real-valued input (the signal strength after the sum) and squashes it to range between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An example code for forward-propagating a single neuron might look as follows.\n",
    "class Neuron(object):\n",
    "  # ... \n",
    "  def forward(self, inputs):\n",
    "    \"\"\" assume inputs and weights are 1-D numpy arrays and bias is a number \"\"\"\n",
    "    cell_body_sum = np.sum(inputs * self.weights) + self.bias\n",
    "    firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) ## sigmoid activation function\n",
    "    return firing_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single neuron as a linear classifier\n",
    "### Binary Softmax classifier.\n",
    "* we can interpret  ${\\displaystyle \\sigma\\left(\\sum_{i}w_{i}x_{i}+b\\right)}$ to be the probability of one of the classes $P(y_i = 1 \\mid x_i; w)$. The probability of the other class would be $P(y_i = 0 \\mid x_i; w) = 1 - P(y_i = 1 \\mid x_i; w)$. \n",
    "* With this interpretation, we can formulate the <span class=\"girk\">cross-entropy loss</span> as we have seen in the Linear Classification section, and optimizing it would lead to a <span class=\"girk\">binary Softmax classifier</span> (also known as logistic regression).\n",
    "\n",
    "### Binary SVM classifier\n",
    "* Alternatively, we could attach a <span class=\"girk\">max-margin hinge loss</span> to the output of the neuron and train it to become a binary Support Vector Machine.\n",
    "\n",
    "### Regularization interpretation\n",
    "* The regularization loss in both SVM/Softmax cases could in this biological view be interpreted as **<span class=\"girk\">gradual forgetting</span>**, since it would have the effect of driving all synaptic weights ww towards zero after every parameter update.\n",
    "\n",
    "<span class=\"burk\">A single neuron can be used to implement a binary classifier (e.g. binary Softmax or binary SVM classifiers)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commonly used activation functions\n",
    "* Sigmoid\n",
    "* Tanh\n",
    "* ReLU\n",
    "* Leaky ReLU\n",
    "* Maxout\n",
    "* TLDR\n",
    "\n",
    "### Sigmoid\n",
    "* mathematical form $\\sigma(x) = 1 / (1 + e^{-x})$\n",
    "* it takes a real-valued number and “squashes” it into range between 0 and 1. In particular, large negative numbers become 0 and large positive numbers become 1.\n",
    "* frequent use historically since it has a nice interpretation.\n",
    "\n",
    "<img class =“left” src='http://cs231n.github.io/assets/nn1/sigmoid.jpeg' width='350'/>\n",
    "\n",
    "#### 对 sigmoid 的分析：\n",
    "<span class=\"burk\">Sigmoids saturate and kill gradients</span>.\n",
    "* A very undesirable property of the sigmoid neuron is that when the neuron’s activation saturates at either tail of 0 or 1, <span class=\"girk\">the gradient at these regions is almost zero</span>. \n",
    "* Recall that during backpropagation, this (local) gradient will be multiplied to the gradient of this gate’s output for the whole objective. Therefore, if the local gradient is very small, it will effectively “kill” the gradient and almost no signal will flow through the neuron to its weights and recursively to its data.\n",
    "* Additionally, one must pay extra caution when initializing the weights of sigmoid neurons to prevent saturation.  For example, if the initial weights are too large then most neurons would become saturated and the network will barely learn.\n",
    "    \n",
    "<span class=\"burk\">Sigmoid outputs are not zero-centered</span>.\n",
    "* undesirable since neurons in later layers of processing in a Neural Network (more on this soon) would be receiving data that is not zero-centered\n",
    "* This has implications on the dynamics during gradient descent, because if the <span class=\"girk\">data coming into a neuron is always positive</span> (e.g. $x>0$ elementwise in $f=w^Tx+b$)), then the gradient on the weights $w$ will during <span class=\"girk\">backpropagation become either all be positive, or all negativ</span>e (depending on the gradient of the whole expression ff).\n",
    "* However, notice that once these gradients are added up across a batch of data the final update for the weights can have variable signs, somewhat mitigating this issue. Therefore, this is an inconvenience but it has less severe consequences compared to the saturated activation problem above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh\n",
    "\n",
    "<img class =“right” src='http://cs231n.github.io/assets/nn1/tanh.jpeg' width='350'/>\n",
    "* It squashes a real-valued number to the range $[-1, 1]$. \n",
    "* Like the sigmoid neuron, its <span class=\"girk\">activations saturate</span>, but unlike the sigmoid neuron its <span class=\"girk\">output is zero-centered</span>. \n",
    "* Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity.\n",
    "* Relationship with sigmoid $\\tanh(x)=2\\sigma(2x)−1.$\n",
    "\n",
    "### ReLU\n",
    "<img src='http://cs231n.github.io/assets/nn1/relu.jpeg' width='350'/>\n",
    "* The Rectified Linear Unit (ReLU) has become very popular in the last few years.\n",
    "* It computes the function $f(x)=\\max(0,x)$. In other words, the activation is <span class=\"girk\">simply thresholded at zero</span>.\n",
    "\n",
    "\n",
    "分析:\n",
    "* (+) It was found to <span class=\"girk\">greatly accelerate</span> (e.g. a factor of 6 in [Krizhevsky et al.](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)) the convergence of stochastic gradient descent compared to the sigmoid/tanh functions. It is argued that this is due to its <span class=\"girk\">linear, non-saturating form</span>.\n",
    "* (+) Compared to tanh/sigmoid neurons that involve expensive operations (exponentials, etc.), the ReLU can be <span class=\"girk\">implemented by simply thresholding a matrix of activations at zer</span>o.\n",
    "* (-) Unfortunately, ReLU units can be fragile 易损的 during training and can “die”.\n",
    "    * For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that <span class=\"girk\">the neuron will never activate on any datapoint again</span>. 全为 0 了呗.\n",
    "    * If this happens, then the gradient flowing through the unit will forever be zero from that point on.\n",
    "    * That is, the ReLU units can <span class=\"girk\">irreversibly die</span> during training since they can get knocked off the data manifold.\n",
    "        * For example, you may find that <span class=\"girk\">as much as 40% of your network can be “dead”</span> (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high. \n",
    "    * With a <span class=\"girk\">proper setting of the learning rate</span> this is less frequently an issue.\n",
    "\n",
    "### Leaky ReLU\n",
    "<img src='http://cs231n.github.io/assets/nn1/alexplot.jpeg' width='350'/>\n",
    "* <span class=\"girk\">Leaky ReLUs</span> are one attempt to fix the “dying ReLU” problem. \n",
    "* Instead of the function being zero when $x < 0$, a leaky ReLU will instead have a small negative slope (of 0.01, or so).\n",
    "* Mathimatic formula　$$f(x) = \\mathbb{1}(x < 0) (\\alpha x) + \\mathbb{1}(x>=0) (x)$$ where $\\alpha$ is a small constant.\n",
    "\n",
    "\n",
    "* Some people report success with this form of activation function, but <span class=\"girk\">the results are not always consistent</span>. \n",
    "    * The slope in the negative region can also be made into a parameter of each neuron, as seen in PReLU neurons, introduced in [Delving Deep into Rectifiers](http://arxiv.org/abs/1502.01852), by Kaiming He et al., 2015. Like almost died neurons.\n",
    "    * However, <span class=\"girk\">the consistency of the benefit across tasks is presently unclear</span>.\n",
    "\n",
    "\n",
    "### Maxout\n",
    "* Maxout neuron (introduced recently by Goodfellow et al.) that generalizes the ReLU and its leaky version.\n",
    "$$ \\max(w_1^Tx+b_1, w_2^Tx + b_2) $$\n",
    "\n",
    "* Notice that both ReLU and Leaky ReLU are a special case of this form (for example, for ReLU we have $w1,b1=0$).\n",
    "\n",
    "\n",
    "分析:\n",
    "* (+) The Maxout neuron therefore <span class=\"girk\">enjoys all the benefits of a ReLU unit</span> (<span class=\"girk\">linear regime of operation, no saturation</span>) and does not have its drawbacks (dying ReLU).\n",
    "* (-) However, unlike the ReLU neurons it <span class=\"girk\">doubles the number of parameters</span> for every single neuron, leading to a high total number of parameters.\n",
    "\n",
    "### Conclusion\n",
    "* It is very rare to mix and match different types of neurons in the same network, even though there is no fundamental problem with doing so.\n",
    "\n",
    "* What neuron type should I use?\n",
    "    * (1) Use the ReLU non-linearity, (2) be <span class=\"girk\">careful</span> with your <span class=\"girk\">learning rates</span> and (3) possibly <span class=\"girk\">monitor</span> the fraction of <span class=\"girk\">“dead” units</span> in a network.\n",
    "    * If this concerns you, give Leaky ReLU or Maxout a try. \n",
    "    * Never use sigmoid.\n",
    "    * Try tanh, but expect it to work worse than ReLU/Maxout.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network architectures\n",
    "\n",
    "### Layer-wise organization\n",
    "\n",
    "#### Neural Networks as neurons in graphs\n",
    "<span class=\"burk\">Neural Networks as neurons in graphs</span>: \n",
    "* collections of neurons that are connected in an acyclic graph.\n",
    "* acyclic 非周期的 graph: Cycles are not\n",
    "* the most common layer type is the **<span class=\"girk\">fully-connected layer</span>** in which neurons between two adjacent layers are fully pairwise connected, but neurons within a single layer share no connections. \n",
    "\n",
    "<img src='http://cs231n.github.io/assets/nn1/neural_net2.jpeg' width='400' \\>\n",
    "Figure. A <span class=\"girk\">3-layer neural network (<span class=\"girk\"><span class=\"girk\">hidden layer + output layer</span></span>)</span> with three inputs, two hidden layers of 4 neurons each and one output layer.\n",
    "\n",
    "#### Naming conventions\n",
    "* when we say <span class=\"girk\">N-layer neural network</span>, we do not count the input layer. only hidden layer + output layer.\n",
    "* a single-layer neural network describes a network with no hidden layers (input directly mapped to output).\n",
    "    * in this sense, logistic regression or SVMs are simply a special case of single-layer Neural Networks.\n",
    "    * You may also hear these networks interchangeably referred to as “Artificial Neural Networks” (ANN) or “Multi-Layer Perceptrons” (MLP)\n",
    "    \n",
    "    \n",
    "#### Output layer\n",
    "* the output layer neurons most commonly do not have an activation function (or you can think of them as having a linear identity activation function). # 难道 softmax 不是 non-linear  function 吗?\n",
    "* [意义统一] the last output layer is usually taken to <span class=\"girk\">represent the class scores</span> (e.g. in classification), which are arbitrary real-valued numbers, or some kind of real-valued target (e.g. in regression).\n",
    "\n",
    "\n",
    "#### Sizing neural networks.\n",
    "* <span class=\"girk\">(1) the number of neurons</span>, or more commonly <span class=\"girk\">(2) the number of parameters</span> to measure the size of neural networks\n",
    "* 以上图为例:\n",
    "    * 4 + 4 + 1 = 9 neurons\n",
    "    * [3 x 4] + [4 x 4] + [4 x 1] = 12 + 16 + 4 = 32 weights and 4 + 4 + 1 = 9 biases, for a total of 41 learnable parameters.\n",
    "    \n",
    "* modern Convolutional Networks contain on orders of 100 million parameters and are usually made up of approximately 10-20 layers (hence deep learning)\n",
    "* However, as we will see the number of effective connections is significantly greater due to parameter sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example feed-forward computation\n",
    "* 主要计算就是这些: Repeated matrix multiplications interwoven with activation function\n",
    "* One of the <span class=\"girk\">primary reasons</span> that Neural Networks are organized into layers is that this structure makes it <span class=\"girk\">very simple and efficient to evaluate</span> Neural Networks <span class=\"girk\">using matrix vector operations</span>.\n",
    "* All connection strengths <span class=\"girk\">for a layer</span> can be stored in a single matrix.\n",
    "* 以上图为例, the full forward pass of this 3-layer neural network is then simply three matrix multiplications, interwoven with the application of the activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# forward-pass of a 3-layer neural network:\n",
    "f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)\n",
    "x = np.random.randn(3, 1) # random input vector of three numbers (3x1)\n",
    "h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1)\n",
    "h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4x1)\n",
    "out = np.dot(W3, h2) + b3 # output neuron (1x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"burk\">The forward pass of a fully-connected layer corresponds to one matrix multiplication followed by a bias offset and an activation function.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representational power\n",
    "\n",
    "Representational power\n",
    "* NN defines a family of functions that are parameterized by the weights of the network.\n",
    "* What is the representational power of this family of functions?\n",
    "* It turns out that Neural Networks with at <span class=\"girk\">least one hidden layer</span> are <span class=\"girk\">universal approximators</span>.  In other words, the neural network can approximate any continuous function.\n",
    "\n",
    "\n",
    "\n",
    "If one hidden layer suffices to approximate any function, why use more layers and go deeper? \n",
    "* [A]: The answer is that the fact that a two-layer Neural Network is a universal approximator is, while mathematically cute, a relatively weak and <span class=\"girk\">useless statement in practice</span>. \n",
    "* [A]: 反例 $g(x) = \\sum_i c_i \\mathbb{1}(a_i < x < b_i)$ where $a,b,c$ are parameter vectors is also a universal approximator, but no one would suggest that we use this functional form in Machine Learning. \n",
    "* [A]: Neural Networks work well in practice because \n",
    "    * [<span class=\"girk\">表达能力</span>] they compactly express nice, smooth functions that fit well with the statistical properties of data we encounter in practice,\n",
    "    * [<span class=\"girk\">易优化</span>] and are also easy to learn using our optimization algorithms (e.g. gradient descent). \n",
    "* [A]: Similarly, the fact that deeper networks (with multiple hidden layers) can <span class=\"girk\">work better than</span> a single-hidden-layer networks is an <span class=\"girk\">empirical observation</span>, despite the fact that their representational power is equal.\n",
    "\n",
    "\n",
    "\n",
    "Deeper is better ?\n",
    "* [<span class=\"girk\">For DNN</span>] in practice it is often the case that 3-layer neural networks will outperform 2-layer nets, but <span class=\"girk\">going even deeper</span> (4,5,6-layer) <span class=\"girk\">rarely helps much more</span>.  \n",
    "* [<span class=\"girk\">For CNN</span>] the depth has been found to be an <span class=\"girk\">extremely important</span> component for a good recognition system (e.g. <span class=\"girk\">on order of 10 learnable layers</span>). \n",
    "    * One argument for this observation is that <span class=\"girk\">images contain hierarchical structure</span> (e.g. faces are made up of eyes, which are made up of edges, etc.). \n",
    "    * so several layers of processing make intuitive sense for this data domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting number of layers and their sizes\n",
    "\n",
    "#### How do we decide on what architecture to use when faced with a practical problem?\n",
    "Should we use no hidden layers? One hidden layer? Two hidden layers? How large should each layer be?\n",
    "* first, note that as we increase the size and number of layers in a Neural Network, the capacity of the network increases. \n",
    "* We could train three separate neural networks, each with one hidden layer of some size and obtain the following classifiers:\n",
    "\n",
    "![](http://cs231n.github.io/assets/nn1/layer_sizes.jpeg)\n",
    "Figure. Larger Neural Networks can represent more complicated functions.  You can play with these examples in this [ConvNetsJS demo](http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html).\n",
    "\n",
    "* <span class=\"girk\">Overfitting</span> occurs when a model with high capacity <span class=\"girk\">fits the noise</span> in the data instead of the (assumed) underlying relationship. For example, the model with 20 hidden neurons fits all the training data but at the cost of segmenting the space into many disjoint red and green decision regions.\n",
    "* Based on our discussion above, it seems that <span class=\"girk\">smaller neural networks</span> can be preferred if the data is not complex enough to prevent overfitting. However, this <span class=\"girk\">is incorrect</span> - there are many other <span class=\"girk\">preferred ways to prevent overfitting</span> in Neural Networks that we will discuss later (such as <span class=\"girk\">L2 regularization, dropout, input nois</span>e). \n",
    "\n",
    "\n",
    "#### Large NN > Small NN Reasons\n",
    "<span class=\"burk\">Large NN > Small NN Reason #1</span>:\n",
    "* The subtle reason behind this is that <span class=\"girk\">smaller networks are harder to train</span> with local methods such as Gradient Descent.\n",
    "    * It’s clear that their loss functions have relatively few local minima, \n",
    "    * but it turns out that many of these minima are easier to converge to, and that they are bad (i.e. with high loss).\n",
    "* Conversely, bigger neural networks contain significantly more local minima, but these minima turn out to be much better in terms of their actual loss. \n",
    "\n",
    "\n",
    "<span class=\"burk\">Large NN > Small NN Reason #2</span>:\n",
    "* small network the final loss can display a good amount of variance - in some cases you get lucky and converge to a good place but in some cases you get trapped in one of the bad minima. \n",
    "* On the other hand, if you train a large network you’ll start to find many different solutions, but the variance in the final achieved loss will be much smaller. In other words, all solutions are about equally as good, and rely less on the luck of random initialization.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Since Neural Networks are non-convex, it is hard to study these properties mathematically, but some attempts to understand these objective functions have been made, e.g. in a recent paper [The Loss Surfaces of Multilayer Networks](http://arxiv.org/abs/1412.0233).\n",
    "\n",
    "\n",
    "#### Large NN with L2 regularization\n",
    "![](http://cs231n.github.io/assets/nn1/reg_strengths.jpeg)\n",
    "Figure. The effects of regularization strength: Each neural network above has 20 hidden neurons, but changing the regularization strength makes its final decision regions smoother with a higher regularization. You can play with these examples in this [ConvNetsJS demo](http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html).\n",
    "\n",
    "#### Takeaway notes\n",
    "* should not be using smaller networks because you are afraid of overfitting. \n",
    "* Instead, you should use as big of a neural network as your computational budget allows, and use other regularization techniques to control overfitting\n",
    "\n",
    "\n",
    "## Summary\n",
    "* We introduced a very coarse model of a biological <span class=\"girk\">neuron</span>\n",
    "* We discussed several types of <span class=\"girk\">activation functions</span> that are used in practice, with ReLU being the most common choice\n",
    "* We introduced <span class=\"girk\">Neural Networks</span> where neurons are connected with <span class=\"girk\">Fully-Connected layers</span> where neurons in adjacent layers have full pair-wise connections, but neurons within a layer are not connected.\n",
    "* We saw that this layered architecture enables very efficient evaluation of Neural Networks based on matrix multiplications interwoven with the application of the activation function.\n",
    "* We saw that that Neural Networks are <span class=\"girk\">universal function approximators</span>, but we also discussed the fact that this property has little to do with their ubiquitous use. They are used because they make certain “right” assumptions about the functional forms of functions that come up in practice.\n",
    "* We discussed the fact that larger networks will always work better than smaller networks, but their higher model capacity must be appropriately addressed with stronger regularization (such as higher weight decay), or they might overfit. We will see more forms of regularization (especially dropout) in later sections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "toc_cell": true,
   "toc_position": {
    "height": "330px",
    "left": "793px",
    "right": "20px",
    "top": "67.6px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
