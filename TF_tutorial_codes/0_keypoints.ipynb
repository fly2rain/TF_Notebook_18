{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#常见网络-VGG,-RestNet,-SqueezeNet,-GoogLeNet\" data-toc-modified-id=\"常见网络-VGG,-RestNet,-SqueezeNet,-GoogLeNet-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span><a href=\"\" target=\"_blank\">常见网络 VGG, RestNet, SqueezeNet, GoogLeNet</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#VGG-16,-VGG-19-developed-in-2014\" data-toc-modified-id=\"VGG-16,-VGG-19-developed-in-2014-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span><a href=\"https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/\" target=\"_blank\">VGG-16, VGG-19 developed in 2014</a></a></span></li><li><span><a href=\"#ResNet\" data-toc-modified-id=\"ResNet-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span><a href=\"https://github.com/KaimingHe/deep-residual-networks\" target=\"_blank\">ResNet</a></a></span></li><li><span><a href=\"#有意思的小模块\" data-toc-modified-id=\"有意思的小模块-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span><a href=\"\" target=\"_blank\">有意思的小模块</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Global-Average-Pooling-Layers-(i.e.-fully-convolution-network,-FCN)\" data-toc-modified-id=\"Global-Average-Pooling-Layers-(i.e.-fully-convolution-network,-FCN)-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span><a href=\"https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/\" target=\"_blank\">Global Average Pooling Layers (i.e. fully convolution network, FCN)</a></a></span></li><li><span><a href=\"#fully-convolution-network,-FCN\" data-toc-modified-id=\"fully-convolution-network,-FCN-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span><a href=\"\" target=\"_blank\">fully convolution network, FCN</a></a></span></li><li><span><a href=\"#FCN-over-CNN\" data-toc-modified-id=\"FCN-over-CNN-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span><a href=\"https://www.quora.com/What-are-the-advantages-of-Fully-Convolutional-Networks-over-CNNs\" target=\"_blank\">FCN over CNN</a></a></span></li><li><span><a href=\"#3x3,-5x5,-7x7's-CNN-filter's-effects\" data-toc-modified-id=\"3x3,-5x5,-7x7's-CNN-filter's-effects-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span><a href=\"https://www.quora.com/In-convolutional-neural-networks-what-effect-does-the-size-e-g-3x3-5x5-7x7-of-the-convolution-kernel-have-on-the-architecture-of-the-convolutional-neural-networks\" target=\"_blank\">3x3, 5x5, 7x7's CNN filter's effects</a></a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [常见网络 VGG, RestNet, SqueezeNet, GoogLeNet]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网页: https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/\n",
    "\n",
    "* The <span class=\"girk\">state-of-the-art pre-trained networks</span> included in the <span class=\"girk\">Keras</span> core library represent some of the highest performing Convolutional Neural Networks on the ImageNet challenge over the past few years. \n",
    "* These networks also demonstrate a strong ability to generalize to images outside the ImageNet dataset via transfer learning, such as feature extraction and fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from keras.applications import ResNet50 # ❤️\n",
    "from keras.applications import InceptionV3 # ❤️\n",
    "from keras.applications import Xception # TensorFlow ONLY\n",
    "from keras.applications import VGG16\n",
    "from keras.applications import VGG19\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Lines 2-13** import our required Python packages. As you can see, most of the packages are part of the Keras library.\n",
    "\n",
    "* **Lines 2-6** handle importing the Keras implementations of ResNet50, Inception V3, Xception, VGG16, and VGG19, respectively.\n",
    "\n",
    "* **Line 7** gives us access to the imagenet_utils  sub-module, a handy set of convenience functions that will make pre-processing our input images and decoding output classifications easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [VGG-16, VGG-19 developed in 2014](https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/)\n",
    "\n",
    "###### 网络结构\n",
    "![](https://www.pyimagesearch.com/wp-content/uploads/2017/03/imagenet_vgg16.png)\n",
    "Figure 1: A visualization of the VGG architecture (source).\n",
    "\n",
    "###### 特点:\n",
    "* its simplicity, using only <span class=\"girk\">3×3 convolutional layers</span> stacked on top of each other in increasing depth.\n",
    "* <span class=\"girk\">Reducing volume size</span> is handled by max pooling.\n",
    "\n",
    "\n",
    "![](https://www.pyimagesearch.com/wp-content/uploads/2017/03/imagenet_vggnet_table1.png)\n",
    "Figure 2: Table 1 of Very Deep Convolutional Networks for Large Scale Image Recognition, Simonyan and Zisserman (2014).\n",
    "* The smaller networks converged and were then used as initializations for the larger, deeper networks — this process is called <span class=\"burk\">pre-training</span>.\n",
    "\n",
    "\n",
    "##### there are two major drawbacks with VGGNet:\n",
    "* It is painfully slow to train.\n",
    "* The network architecture weights themselves are quite large (in terms of disk/bandwidth).\n",
    "    * Due to <span class=\"girk\">its depth and number of fully-connected nodes</span>, VGG is over 533MB for VGG16 and 574MB for VGG19. This makes deploying VGG a tiresome task.\n",
    "    * smaller network architectures are often more desirable (such as SqueezeNet, GoogLeNet, etc.).\n",
    "\n",
    "##### Code\n",
    "1. VGG in TensorFlow: http://www.cs.toronto.edu/~frossard/post/vgg16/\n",
    "2. vgg16.py: https://www.cs.toronto.edu/~frossard/vgg16/vgg16.py\n",
    "3. keras-vggface: https://github.com/rcmalli/keras-vggface/blob/master/keras_vggface/models.py\n",
    "4. tensorflow-vgg: https://github.com/machrisaa/tensorflow-vgg/blob/master/vgg16.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ResNet](https://github.com/KaimingHe/deep-residual-networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ResNet-50: http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006\n",
    "* ResNet-101: http://ethereon.github.io/netscope/#/gist/b21e2aae116dc1ac7b50\n",
    "* ResNet-152: http://ethereon.github.io/netscope/#/gist/d38f3e6091952b45198b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [有意思的小模块]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Global Average Pooling Layers (i.e. fully convolution network, FCN)](https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/)\n",
    "\n",
    "##### General CNN Structure\n",
    "\n",
    "* General CNN structure for the image classification task:\n",
    "    * repeated blocks of convolution and max pooling layers, \n",
    "    * followed by two or more densely connected layers\n",
    "    * The final dense layer has a softmax activation function and a node for each potential object category.\n",
    "    \n",
    "![](https://alexisbcook.github.io/assets/vgg16.png)\n",
    "The VGG network to illustrate the general example of the CNN.\n",
    "\n",
    "\n",
    "```python\n",
    "python -c 'from keras.applications.vgg16 import VGG16; VGG16().summary()'\n",
    "```\n",
    "![](https://alexisbcook.github.io/assets/vgg16_keras.png)\n",
    "\n",
    "\n",
    "\n",
    "##### Drawbacks of the FC\n",
    "* Notice that most of the parameters in the model belong to the fully connected (FC) layers!\n",
    "* an architecture like this has the risk of overfitting to the training dataset. In practice, dropout layers are used to avoid overfitting.\n",
    "\n",
    "##### Why GAP?\n",
    "* In the last few years, experts have turned to global average pooling (GAP) layers to minimize overfitting by <span class=\"girk\">reducing the total number of parameters</span> in the model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* Similar to max pooling layers, <span class=\"burk\">GAP</span> layers are used to <span class=\"girk\">reduce the spatial dimensions</span> of a three-dimensional tensor. \n",
    "    * However, GAP layers perform a more extreme type of dimensionality reduction, where a tensor with dimensions $h×w×d$ is reduced in size to have dimensions $1×1×d$. \n",
    "    * GAP layers reduce each $h×w$ feature map to <span class=\"girk\">a single number</span> by simply <span class=\"girk\">taking the average of all $hw$ values</span>.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](https://alexisbcook.github.io/assets/global_average_pooling.png)\n",
    "\n",
    "\n",
    "* Example of the GAP in ResNet.\n",
    "    * ResNet-50: http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [fully convolution network, FCN]()\n",
    "* FCN 最开始是在 segmentation 中出现的, 现在 image classification 中也存在了."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [FCN over CNN](https://www.quora.com/What-are-the-advantages-of-Fully-Convolutional-Networks-over-CNNs)\n",
    "* Input image size:\n",
    "    * CNN: <span class=\"girk\">fully connected layer</span> expects inputs of a <span class=\"girk\">certain size</span>, which is why in architectures like AlexNet, you must provide input images of a certain size (224x224).\n",
    "    * FCN: can take images of virtually any size.\n",
    "* Spatial information:\n",
    "    * CNN: Fully connected layer generally causes **loss of spatial information \n",
    "        * because its “fully connected”: all output neurons are connected to all input neurons.\n",
    "        * can’t be used for segmentation.\n",
    "        \n",
    "        \n",
    "* Computational cost and representation power:\n",
    "    * CNN: most of the memory is used in the early convolutional layers. And, most of the parameters are in the last Fully-connected layers.\n",
    "    * Researchers are beginning to <span class=\"girk\">favor having a greater number</span> of convolutional layers, tending towards fully convolutional networks for everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [3x3, 5x5, 7x7's CNN filter's effects](https://www.quora.com/In-convolutional-neural-networks-what-effect-does-the-size-e-g-3x3-5x5-7x7-of-the-convolution-kernel-have-on-the-architecture-of-the-convolutional-neural-networks)\n",
    "\n",
    "##### Convolutional neural networks work on 2 assumptions:\n",
    "* Low level features are local\n",
    "* What's useful in one place will also be useful in other places.\n",
    "\n",
    "<span class=\"girk\">Kernel size</span> should be determined by how strongly we believe in those assumptions for the problem at hand.\n",
    "* <span class=\"burk\">extreme case #1</span>: low level features are per-pixel, and they don't affect neighbouring pixels at all, and that we should apply the same operation to all pixels.\n",
    "* <span class=\"burk\">extreme case #2</span>: kernels the size of the entire image. In this case the CNN essentially becomes fully connected, and stops being a CNN, and we are no longer making any assumption on low level feature locality.\n",
    "* In practice, this is often done by just trying a few kernel sizes, and see what works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Data preprocessing ]()\n",
    "* http://cs231n.github.io/neural-networks-2/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
